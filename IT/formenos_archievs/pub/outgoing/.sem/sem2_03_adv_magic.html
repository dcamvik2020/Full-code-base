<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Урок 3: Фокусы целочисленных и вещественных типов - Продвинутая магия</title>
</head>
<body>
<div style="white-space: pre-wrap;">
<ul>
<li>Что касается раздела про продвинутую магию, в этот раз в тему очень хорошо ложится <a href="http://habrahabr.ru/post/156593/" target="_blank">статейка</a> про истинную природу <s>Силы</s> целочисленных типов.
Мы с вами уже сталкивались с ней, но сейчас было бы весьма кстати вспомнить её.
Намного ли тип <tt>char</tt> меньше типа <tt>long</tt>?
Всё, что вы всю жизнь хотели узнать о простейших типах языка Си, но боялись спросить. О сколько нам открытий чудных готовит просвещенья дух!
</li>
<li>Однако и вещественные типы нас иногда могут удивить.
Я вам рассказывал, что тип <tt>long double</tt> соответствует расширенной точности чисел с плавающей запятой, а в своих табличках писал, что он занимает 10 байт (80 бит).
Да, так и было, например, в Borland'овских компиляторах, которые давно почили в бозе. Однако даже в послушном Linux GCC, чтящем всевозможные стандарты, <tt>sizeof(long double)</tt> выдаёт 12 <s>байт</s> char'ов в 32-битных компиляторах и аж 16 char'ов в 64-битных. А Microsoft Visual Studio 2008 (32-битная, до сих пор живущая в компьютерных классах) выдаёт и вовсе 8. (И кое-кто из вас уже с этой проблемой столкнулся в задаче size.)
Нет ли здесь противоречия?
Итак, как соотносятся между собой короткое, длинное и расширенное представление вещественных чисел, и причём тут Intel x87 и <a href="https://en.wikipedia.org/wiki/IEEE_floating_point" target="_blank">IEEE 754 floating point standard</a>?
Формат вещественных чисел, в том числе одинарной, двойной и повышенной точности зафиксирован в стандарте <a href="https://ru.wikipedia.org/wiki/IEEE_754-2008" target="_blank">IEEE 754</a>, однако Intel'овский математический сопроцессор x87 появился несколько ранее, и именно он и стал де-факто законодателем мод в вещественной арифметике.
А вот первые Стандарты Си появились позднее, и там вещественные типы должны были покрывать и другие сопроцессоры, некоторые из которых были даже попроизводительнее Intel'овского x87. Не говоря уже о том, что математического сопроцессора могло и не быть физически в машине, и тогда все вещественные вычисления эмулировались (что было, конечно, значительно менее эффективно, но тем не менее всё равно работало), однако в программах <tt>double</tt> оставался <tt>double</tt>'м, а <tt>long double</tt> - <tt>long double</tt>'м.
Ещё раз повторю, что секрет такой распространённости языка Си кроется в его переносимости на другие платформы, а секрет переносимости - в том числе и в том, что размеры типов были не зафиксированы точно. Были лишь некоторые ограничения, которые можно посмотреть, например, <a href="https://ru.wikipedia.org/wiki/Типы_данных_в_C" target="_blank">здесь</a>.
Но обычно всё-таки короткому представлению (одинарной точности) соответствовал <tt>float</tt>, длинному представлению (двойной точности) - <tt>double</tt>, а расширенному представлению - <tt>long double</tt>. Однако тот же Стандарт пишет, что <tt>long double</tt> мог использоваться и для других форматов. И если явно не указано, какой именно, то <tt>long double</tt> мог быть даже эквивалентен простому <tt>double</tt>.
Можете почитать на аглицком грустную историю жизни типа <tt>long double</tt> вот <a href="https://en.wikipedia.org/wiki/Long_double" target="_blank">здесь</a>.
Что ж, именно поэтому Visual Studio и считает, что <tt>long double</tt> и <tt>double</tt> - одно и то же. Имеет право, блин. :-( Однако это поведение может регулироваться опциями компилятора.
А что же законопослушный Linux GCC? А он просто-напросто выравнивает размер типа данных по границе, кратной размеру машинного слова (4 байт в 32-битных и 8 байт в 64-битных), т.е. добивает его неиспользуемыми нулями (<i>padding</i>) с той или с другой стороны. Т.е. это вроде как и не противоречит всяким стандартам. Но опять же, можно опциями компилятора настроить требуемую точность. А если компилятора нету, а есть только голый ассемблер, то можно определёнными битиками регистра управления FPU задать точность и округление вычислений.
</li>
<li>Ну и в заключение попугаю вас и напомню, какие нежизнеспособные гибридные мутанты получаются от скрещивания дикого свободного OpenSource и вульгарной проприетарщины.
Многие из вас используют QtCreator или Code::Blocks, реже Eclipse - под Виндой. Но эти продукты обычно (но не всегда) используют GCC Toolchain. А порт GCC под Винду - это MinGW. А MinGW не имеет своего <i>runtime</i> (т.е. функций стандартной библиотеки LibC - <tt>printf</tt>, <tt>scan</tt>f и т.д.), поэтому использует системный. А системный - это msvcrt.dll. А msvcrt.dll - это динамическая версия стандартной библиотеки компилятора ... угадайте, какого? Правильно, молодцы, Microsoft Visual C++. Причём не абы какой версии, а самой что ни на есть 6.0 (1998 года). По крайней мере, так было до последних релизов Windows.
И получается, что MinGW генерирует правильный код для типов <tt>long double</tt> (компилятор-то GCC-шный), но вот при вводе и выводе начинаются адские страсти-мордасти, т.е. вижуалстудиевские <tt>printf</tt> и <tt>scanf</tt> наивно полагают, что <tt>long double</tt> - это просто <tt>double</tt> размера 8 байт. Со всеми вытекающими, выпадающими, вываливающимися и взрывающимися.
Похожая история происходит, если вы задумали переменную типа <tt>char</tt> ввести/вывести не как символ, а как небольшое целое число. А вижуалстудиевские <tt>printf</tt> и <tt>scanf</tt> не принимают спецификаторы <tt>%hhd</tt>, <tt>%hhu</tt> и т.п.
</li>
</ul>
</div>
<div id="comments"></div>
<script src="comments.js"></script>
</body>
</html>
